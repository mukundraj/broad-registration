""" Script to process annodata files generated by R script (which generates the
annodata from Jonah's qs object). Output expected to be zarr file to be used
downstream for interactive visualization.

Usage:

python s8_process_integ_mat.py
    inp: data root
    inp: path to bead_ccf_labels_allbds with bead metadata in csv format
    inp: path to bead_ccf_coords_allbds with bead coords in chuck space in csv format
    inp: input to integrated_mats folder with processed gene counts in annodata h5ad format
    inp: path to nissl images folder
    inp: path to atlas wireframe images folder
    out: output dir to write gene jsons to

Usage example:

python src/python/scripts/v2/s8_process_integ_mat.py \
    ~/Desktop/work/data/mouse_atlas \
    /data_v3_nissl_post_qc/s7_annotations/bead_ccf_labels_allbds \
    /data_v3_nissl_post_qc/s3_registered_ss/chuck_img_coords_allbds \
    /data_v3_nissl_post_qc/s8_raw_data/integrated_mats \
    /data_v3_nissl_post_qc/s0_start_formatted_data/transformed_hz_png \
    /data_v3_nissl_post_qc/s7_annotations/allen_labels_imgs/wireframe \
    /data_v3_nissl_post_qc/s9_analysis/gene_jsons

Created by Mukund on 2022-05-04

References:
- on sparse and zarr https://github.com/zarr-developers/zarr-python/issues/424

gsutil -m cp -r ~/Desktop/work/data/mouse_atlas/data_v3_nissl_post_qc/s9_analysis/gene_jsons gs://ml_portal2/test_data2/

gsutil -m cp -r ~/Desktop/work/data/mouse_atlas/data_v3_nissl_post_qc/s9_analysis/gene_jsons/puck1 gs://ml_portal2/test_data2/gene_jsons/

"""

import sys
import anndata as ann
import numpy as np
from scipy.sparse import csr_matrix, hstack
import zarr
import json
from produtils import dprint
import os
import shutil
import csv
import subprocess
import time
from multiprocessing import Pool
import gc

data_root = sys.argv[1]
label_data_folder = data_root+sys.argv[2]
ip_folder_chuck_coords = data_root+sys.argv[3]
in_folder = data_root+sys.argv[4]
ip_folder_nissl = data_root+sys.argv[5]
ip_folder_atlas = data_root+sys.argv[6]
op_folder = data_root+sys.argv[7]

genes_list = ['Pcp4', 'Calb1', 'Gng13', 'Gabra6',
              'Mbp', 'Plp1', 'Mag',
              'Myoc', 'Agt', 'Gfap', 'Slc1a3', 'Aqp4',
              'Dcn', 'Flt1',
              'Rarres2', 'Foxj1']
genes_list.extend(['Xpo7', 'Cul1', 'Herc1', 'Rb1cc1', 'Setd1a','Trio', 
                   'Cacna1g', 'Sp4', 'Gria3', 'Grin2a',
                   'Slc17a7', 'Dsp','Gad2'])

# genes_list = ['slc17a7', 'Dsp', 'Gad2']
# genes_list = ['Pcp4']

# for pid in range(1,42,2):
def process_pid(pid):

    dprint(f'starting pid {pid}..................')
    apid = pid
    if (pid==5 or pid==77 or pid==167):
        apid = pid - 2 ## adjusted pid todo: modify viewer to not require this adjustment

    # ip_coords_file  = f'{in_folder}/ad_coords_{str(apid)}.h5ad'
    ip_counts_file  = f'{in_folder}/ad_counts_{str(apid)}.h5ad'

    counts = ann.read_h5ad(ip_counts_file)
    # coords = ann.read_h5ad(ip_coords_file)

    counts_X = csr_matrix(counts.X).transpose()
    # coords_X = csr_matrix(coords.X)
    # coords_dense_np = np.array(coords_X.todense())
    # xs = coords_dense_np[:, 0].astype(int).tolist()
    # ys = coords_dense_np[:, 1].astype(int).tolist()
    # zs = coords_dense_np[:, 2].astype(int).tolist()
    # data = {'x': xs,
    #         'y': ys,
    #         'z': zs}
    # json_string = json.dumps(data)

    # reading csv for label data

    nis_id_str = str(apid).zfill(3)
    labels_csv_file = f'{label_data_folder}/allen_anno_data_{nis_id_str}.csv'
    # dprint(labels_csv_file)
    region_names = []
    out_tissue = []
    with open(labels_csv_file, newline='\n') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            region_names.append(row[4])
            out_tissue.append(row[8])

    # dprint(len(region_names), len(out_tissue))
    puck_folder = f'{op_folder}/puck{pid}'
    if os.path.exists(puck_folder):
        shutil.rmtree(puck_folder)
    os.mkdir(puck_folder)


    # get chuck space img coords
    chuck_sp_img_coords_file = f'{ip_folder_chuck_coords}/chuck_sp_img_coords_{nis_id_str}.csv'
    chuck_sp_img_coords = []
    with open(chuck_sp_img_coords_file, newline='\n') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            chuck_sp_img_coords.append([int(row[0]), int(row[1])])

    # copy nissl and atlas images to puck directory
    from_nis_file = f'{ip_folder_nissl}/nis_{nis_id_str}.png'
    to_nis_file = f'{puck_folder}/nis_{nis_id_str}.png'
    from_atlas_file = f'{ip_folder_atlas}/chuck_sp_wireframe_{nis_id_str}.png'
    to_atlas_file = f'{puck_folder}/chuck_sp_wireframe_{nis_id_str}.png'
    # dprint(from_atlas_file)
    # dprint(to_atlas_file)
    subprocess.run(["cp", from_nis_file, to_nis_file])
    subprocess.run(["cp", from_atlas_file, to_atlas_file])

    # writing coords tsv
    coords_csv_name = f'{puck_folder}/coords.csv'
    # dprint(np.shape(coords_dense_np))
    # dprint(coords_csv_name, pid, apid)
    # np.savetxt(coords_csv_name, np.array([xs,ys,zs]).T, fmt='%i', header="x,y,z", comments='', delimiter=",")
    in_tissue_inds = []
    with open(coords_csv_name, 'w') as outfile:
        writer = csv.writer(outfile, delimiter=':')
        # writer.writerow(['x', 'y', 'z', 'rname'])
        writer.writerow(['x', 'y', 'rname'])
        for i, status in enumerate(out_tissue):
            if (status=='F'):
                # writer.writerow([xs[i], ys[i], zs[i], region_names[i]])
                writer.writerow([chuck_sp_img_coords[i][0], chuck_sp_img_coords[i][1], region_names[i]])
                in_tissue_inds.append(i)

    # json_file = f'{puck_folder}/coords.json'
    # Directly from dictionary
    # with open(json_file, 'w') as outfile:
    #     json.dump(data, outfile, separators=(',', ':'))

    genes = list(counts.obs_names)

    geneOptions_json_file = f'{puck_folder}/geneOptions.json'
    gene_options_dict = {'geneOptions':genes}
    with open(geneOptions_json_file, 'w') as outfile:
        json.dump(gene_options_dict, outfile, separators=(',', ':'))

    # dprint('len genes', len(genes))

    # gene_cnts = {}
    # gene_metadata = {}

    for gene_idx, gene in enumerate(genes):
        if (gene_idx%500==0):
            collected = gc.collect()
            dprint('gene_idx', gene_idx, 'pid', pid, 'collected', collected)
        specific_gene_cnts = counts_X.getcol(gene_idx)
        spec_gene_cnts_dense = np.squeeze(np.array(specific_gene_cnts.todense())).astype(int)
        spec_gene_cnts_dense = spec_gene_cnts_dense[in_tissue_inds]
        # dprint(np.max(spec_gene_cnts_dense))
        gene_metadata={"maxCount":np.max(spec_gene_cnts_dense)}
        gene_cnts=spec_gene_cnts_dense
        gene_csv_name = f'{puck_folder}/gene_{gene}.csv'
        np.savetxt(gene_csv_name, gene_cnts, fmt='%i', header="count", comments='',delimiter=',')

        metadata_json_file = f'{puck_folder}/metadata_gene_{gene}.json'
        with open(metadata_json_file, 'w') as outfile:
            tmp_dict = {'maxCount':str(gene_metadata['maxCount'])}
            json.dump(tmp_dict, outfile, separators=(',', ':'))


    # for gene in genes_list:
    #     gene_idx = genes.index(gene)
    #     specific_gene_cnts = counts_X.getcol(gene_idx)
    #     spec_gene_cnts_dense = np.squeeze(np.array(specific_gene_cnts.todense())).astype(int)
    #     spec_gene_cnts_dense = spec_gene_cnts_dense[in_tissue_inds]
    #     dprint(np.max(spec_gene_cnts_dense))
    #     gene_metadata[gene]={"maxCount":np.max(spec_gene_cnts_dense)}
    #     gene_cnts[gene]=spec_gene_cnts_dense

    # for key in gene_cnts:
    #     gene_csv_name = f'{puck_folder}/gene_{key}.csv'
    #     np.savetxt(gene_csv_name, gene_cnts[key], fmt='%i', header="count", comments='',delimiter=',')

    #     metadata_json_file = f'{puck_folder}/metadata_gene_{key}.json'
    #     with open(metadata_json_file, 'w') as outfile:
    #         dprint(gene_metadata[key])
    #         dprint(key)
    #         tmp_dict = {'maxCount':str(gene_metadata[key]['maxCount'])}
    #         json.dump(tmp_dict, outfile, separators=(',', ':'))

    dprint(f'puck {apid} done')

# start = time.time()
# for pid in range(1,208,2):
#     process_pid(pid)
# end = time.time()
# dprint(f'Total time {end - start} seconds.')

pids = list(range(61,208,2))
if __name__ == '__main__':
    start = time.time()
    with Pool(20) as p:
        p.map(process_pid, pids)
    end = time.time()
    dprint(f'Total time {end - start} seconds.')
